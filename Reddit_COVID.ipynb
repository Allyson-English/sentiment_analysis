{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import textblob as textblob\n",
    "import praw\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import creds\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing NLTK library and associated packaged\n",
    "\n",
    "import nltk\n",
    "nltk.__version__\n",
    "# nltk.download('vader_lexicon')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag_sents\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#Importing textblob to compare sentiment analysis results with those from nltk\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit API wrapper \n",
    "\n",
    "reddit = praw.Reddit(client_id=creds.client_id, \\\n",
    "                     client_secret=creds.client_secret, \\\n",
    "                     user_agent=creds.user_agent, \\\n",
    "                     username=creds.username, \\\n",
    "                     password=creds.password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting daily ID for the Coronavirus daily discussion post\n",
    "\n",
    "subreddit = reddit.subreddit('Coronavirus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gibmed\n"
     ]
    }
   ],
   "source": [
    "#Gets discussion id for that days daily discussion\n",
    "\n",
    "discussion_id = str(list(subreddit.hot(limit=1)))\n",
    "final_id = discussion_id.replace(\"[Submission(id='\",\"\").replace(\"')]\",\"\")\n",
    "print(final_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "#pulls all discussion ids available \n",
    "\n",
    "discussion_ids = []\n",
    "\n",
    "for submission in subreddit.search('Daily Discussion Post'):\n",
    "    if 'Daily Discussion Post' in submission.title:\n",
    "        discussion_ids.append(submission.id)\n",
    "    \n",
    "print(len(discussion_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ghnx5y', 'gh1uuu']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discussion_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for comments in daily threads:  gibmed\n",
      "fqdmvvb\n",
      "fqdkoeo\n",
      "fqdn3t2\n",
      "fqdlmne\n",
      "fqdodh1\n",
      "fqdl9km\n",
      "fqdkkwg\n",
      "fqdo8gd\n",
      "fqdks5d\n",
      "\n",
      "Processing time: 0.01 minutes.\n"
     ]
    }
   ],
   "source": [
    "coronavirus_new_comments(r\"/Users/AllysonEnglish/Active/Active/coronavirus_subreddit_full.json\", discussion_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"/Users/AllysonEnglish/Active/Active/coronavirus_subreddit_full.json\", \"r\") as f:\n",
    "    full_comment_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length1 = 0\n",
    "# for k in full_comment_dict.keys():\n",
    "#     length1 += len(full_comment_dict[k].keys())\n",
    "# length1\n",
    "\n",
    "len(full_comment_dict['ghnx5y'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ghnx5y\n",
      "gh1uuu\n"
     ]
    }
   ],
   "source": [
    "for d in discussion_ids:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gh1uuu\n",
      "gftmeo\n",
      "ghnx5y\n",
      "gekees\n",
      "ggfrvq\n",
      "gf6xnu\n",
      "gdbzvl\n",
      "gdy3p3\n",
      "ga9u7y\n",
      "gcqx67\n",
      "gaw27w\n",
      "g8eo6c\n",
      "gbih0c\n",
      "g912p1\n",
      "gc6tdq\n",
      "g61hda\n",
      "fjla5p\n",
      "g7tugs\n",
      "flad2s\n",
      "g9nks9\n",
      "g6n0i9\n",
      "g5fli1\n",
      "fvz4b2\n",
      "g32elm\n",
      "g3nu29\n",
      "fj1m3e\n",
      "ftmp3u\n",
      "fmfa26\n",
      "g47znm\n",
      "g15wnp\n",
      "fsd9v4\n",
      "g2fafj\n",
      "fn0a59\n",
      "g1sbbp\n",
      "fhzyu5\n",
      "fo5geh\n",
      "fpbug6\n",
      "g78qo4\n",
      "flvdic\n",
      "fyg6tm\n",
      "frr6ik\n",
      "fk5m07\n",
      "fhgnn3\n",
      "fve2wq\n",
      "fiio5o\n",
      "foqmvt\n",
      "fnkb5o\n",
      "fr66o8\n",
      "fz5oik\n",
      "fpxagz\n",
      "ft068e\n",
      "fuu28n\n",
      "g4t5ud\n",
      "g0jcbi\n",
      "fwkvfa\n",
      "fu8saj\n",
      "fxssfx\n",
      "fx6u28\n",
      "fzx3wk\n",
      "fqkk56\n",
      "fg9kfi\n",
      "ffqiyn\n",
      "fkpu59\n",
      "fgxhiy\n",
      "fcr8dy\n",
      "fcfg0w\n",
      "ff8roc\n",
      "fe9i00\n",
      "fevfr5\n",
      "fdr7u4\n",
      "fdcznm\n",
      "fb8ygf\n",
      "fbuxyx\n",
      "fa6etv\n",
      "faowou\n",
      "f9nx69\n",
      "f9az8y\n",
      "f7xqas\n",
      "f8v592\n",
      "f706aq\n",
      "f7h02k\n",
      "f8n49s\n",
      "fere6p\n",
      "fgsu67\n",
      "fgwgiz\n"
     ]
    }
   ],
   "source": [
    "for d in full_comment_dict.keys():\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for disc_id in discussion_ids:\n",
    "    if disc_id not in full_comment_dict.keys():\n",
    "        print(\"Checking for comments in daily threads: \", disc_id)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43350"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 0\n",
    "for a in full_comment_dict.keys():\n",
    "    for b in full_comment_dict.keys():\n",
    "        c += len(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r\"/Users/AllysonEnglish/Active/Active/coronavirus_subreddit_full.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coronavirus_new_comments(json_file_path, discussion_ids, reddit = reddit):\n",
    "    \n",
    "    with open(json_file_path, \"r\") as f:\n",
    "        full_comment_dict = json.load(f)\n",
    "\n",
    "    st = time()\n",
    "\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    count = 0\n",
    "\n",
    "    for disc_id in discussion_ids:\n",
    "        if disc_id not in full_comment_dict.keys():\n",
    "            print(\"Checking for comments in daily threads: \", disc_id)            \n",
    "            disc_dict = {}\n",
    "            full_comment_dict.update({disc_id: disc_dict})\n",
    "            \n",
    "            submission = reddit.submission(disc_id)    \n",
    "            submission.comments.replace_more(limit=0)\n",
    "\n",
    "            for user_comment in submission.comments:\n",
    "                if str(user_comment) not in full_comment_dict[disc_id].keys():\n",
    "                    print(str(user_comment))\n",
    "\n",
    "                    #submission.comments.replace_more(limit=0)\n",
    "                    comment = user_comment.body\n",
    "                    comment = comment.replace('\\n', ' ')\n",
    "                    comment = comment.replace('I\\'m', 'i am').replace('i\\'m', 'i am').replace('i\\'ll', 'i will').replace('I\\'ll', 'i will')\n",
    "                    comment = comment.lower()\n",
    "\n",
    "                    #grab date/ time info for each comment \n",
    "                    utc = submission.created_utc\n",
    "                    dt_object = datetime.fromtimestamp(utc)  \n",
    "\n",
    "                    #performing sentiment analysis\n",
    "                    ss = sid.polarity_scores(comment)\n",
    "\n",
    "                    token_dict = {\"comment_body\" : comment}\n",
    "                    token_dict.update({\"month\":dt_object.strftime(\"%B\")})\n",
    "                    token_dict.update({\"day\" : dt_object.strftime(\"%d\")})\n",
    "                    token_dict.update(ss)\n",
    "                    disc_dict.update({str(user_comment): token_dict})\n",
    "\n",
    "                    full_comment_dict.update({disc_id: disc_dict})\n",
    "\n",
    "    #update json file with new comments\n",
    "#     with open(json_file_path, \"w\") as outfile:\n",
    "#         json.dump(full_comment_dict, outfile, indent = 4)\n",
    "\n",
    "    print(\"\\nProcessing time:\", round((time()-st)/60, 2), \"minutes.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time()\n",
    "\n",
    "print(\"There are\", len(discussion_ids), \"daily discussions to grab comments from.\\n\")\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "for disc_id in discussion_ids:\n",
    "    disc_dict = {}      \n",
    "    \n",
    "    submission = reddit.submission(disc_id)    \n",
    "    submission.comments.replace_more(limit=0)\n",
    "    \n",
    "    print(disc_id, \" number of comments:  \", len(submission.comments))\n",
    "    if disc_id not in comment_dict.keys():\n",
    "        comment_dict[disc_id] = {}\n",
    "        \n",
    "    for user_comment in submission.comments:\n",
    "\n",
    "        if str(user_comment) not in comment_dict[disc_id].keys():\n",
    "\n",
    "            #submission.comments.replace_more(limit=0)\n",
    "            comment = user_comment.body\n",
    "            comment = comment.replace('\\n', ' ')\n",
    "            comment = comment.replace('I\\'m', 'i am').replace('i\\'m', 'i am').replace('i\\'ll', 'i will').replace('I\\'ll', 'i will')\n",
    "            comment = comment.lower()\n",
    "\n",
    "            #grab date/ time info for each comment \n",
    "            utc = submission.created_utc\n",
    "            dt_object = datetime.fromtimestamp(utc)  \n",
    "\n",
    "            #performing sentiment analysis\n",
    "            ss = sid.polarity_scores(comment)\n",
    "\n",
    "\n",
    "            token_dict = {\"comment_body\" : comment}\n",
    "            token_dict.update({\"month\":dt_object.strftime(\"%B\")})\n",
    "            token_dict.update({\"day\" : dt_object.strftime(\"%d\")})\n",
    "            token_dict.update(ss)\n",
    "            disc_dict.update({str(user_comment): token_dict})\n",
    "\n",
    "            comment_dict.update({disc_id: disc_dict})\n",
    "\n",
    "# with open(r\"/Users/AllysonEnglish/Active/Active/coronavirus_subreddit_full.json\", \"w\") as outfile: \n",
    "#     json.dump(comment_dict, outfile, indent = 4)\n",
    "    \n",
    "print(\"\\nProcessing time:\", round((time()-st)/60, 2), \"minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {'jam': 'and bread', 'tea':'with jam', 'doe':'ray me', 'me':'fa so', 'ray':'fa fa',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pull_comment_information():\n",
    "    submission = reddit.submission(disc_id)    \n",
    "    submission.comments.replace_more(limit=0)\n",
    "\n",
    "    for user_comment in submission.comments:\n",
    "        if str(user_comment) not in comment_dict[disc_id].keys():\n",
    "            print(str(user_comment))\n",
    "\n",
    "            #submission.comments.replace_more(limit=0)\n",
    "            comment = user_comment.body\n",
    "            comment = comment.replace('\\n', ' ')\n",
    "            comment = comment.replace('I\\'m', 'i am').replace('i\\'m', 'i am').replace('i\\'ll', 'i will').replace('I\\'ll', 'i will')\n",
    "            comment = comment.lower()\n",
    "\n",
    "            #grab date/ time info for each comment \n",
    "            utc = submission.created_utc\n",
    "            dt_object = datetime.fromtimestamp(utc)  \n",
    "\n",
    "            #performing sentiment analysis\n",
    "            ss = sid.polarity_scores(comment)\n",
    "\n",
    "            token_dict = {\"comment_body\" : comment}\n",
    "            token_dict.update({\"month\":dt_object.strftime(\"%B\")})\n",
    "            token_dict.update({\"day\" : dt_object.strftime(\"%d\")})\n",
    "            token_dict.update(ss)\n",
    "            disc_dict.update({str(user_comment): token_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jam = {'apple': 3, 'blueberry': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict['muffin'] = jam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict['muffin']['apple'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "\n",
    "for disc_id in discussion_ids:\n",
    "    for k in full_comment_dict[disc_id].keys():\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discussion_id = json_data.keys()\n",
    "\n",
    "m_date = []\n",
    "d_date = []\n",
    "negative = []\n",
    "neutral = []\n",
    "positive = []\n",
    "compound = []\n",
    "comment_id = []\n",
    "\n",
    "for k in json_data.keys():\n",
    "    \n",
    "    for y in json_data[k].keys():\n",
    "        comment_id.append(y)\n",
    "    \n",
    "    for x in json_data[k]:\n",
    "        m_date.append(json_data[k][x].get('month'))\n",
    "        d_date.append(json_data[k][x].get('day'))\n",
    "        negative.append(json_data[k][x].get('neg'))\n",
    "        neutral.append(json_data[k][x].get('neu'))\n",
    "        positive.append(json_data[k][x].get('pos'))\n",
    "        compound.append(json_data[k][x].get('compound'))\n",
    "\n",
    "    \n",
    "d = {'comment_id' : comment_id, 'month': m_date, 'date': d_date, 'positive': positive, 'neutral': neutral, 'negative': negative, 'compound': compound}\n",
    "df = pd.DataFrame(data=d)\n",
    "df = df[df.positive != 1.0000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DOM'] = df['month']+df['date']\n",
    "new_df = df.groupby('DOM').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avrg_by_day = df.groupby('DOM').mean().reset_index()\n",
    "\n",
    "def sort_days(x):\n",
    "    if x.startswith('F'):\n",
    "        return 2\n",
    "    elif x.startswith('Mar'):\n",
    "        return 3\n",
    "    elif x.startswith('Ap'):\n",
    "        return 4\n",
    "    elif x.startswith('May'):\n",
    "        return 5\n",
    "\n",
    "avrg_by_day['month_num'] = avrg_by_day['DOM'].apply(sort_days) \n",
    "\n",
    "def day_num(x):\n",
    "    return int(x[-2:])\n",
    "\n",
    "avrg_by_day['day_num'] = avrg_by_day['DOM'].apply(day_num)\n",
    "\n",
    "avrg_by_day.sort_values(['month_num', 'day_num'], inplace = True)\n",
    "avrg_by_day.reset_index(drop=True, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month_num'] = df['DOM'].apply(sort_days)\n",
    "df['day_num'] = df['DOM'].apply(day_num)\n",
    "df.sort_values(['month_num', 'day_num'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['negative'].mean(), df['positive'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_df['compound']\n",
    "x = new_df.index\n",
    "\n",
    "#ploting with matplotlib\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,4))\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(x_labels)\n",
    "ax.scatter(x, y)\n",
    "\n",
    "\n",
    "z = np.polyfit(x, y, 1)\n",
    "p = np.poly1d(z)\n",
    "ax.plot(x,p(x),\"r--\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workdesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating csv with new inputs\n",
    "\n",
    "old_df = pd.read_csv(r'/Users/AllysonEnglish/Active/Active/sentiment_analysis_draft1.csv')\n",
    "to_add = df[~df['discussion_id'].isin(old_df['discussion_id'])]\n",
    "to_add.to_csv(r'/Users/AllysonEnglish/Active/Active/sentiment_analysis_draft1.csv', mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why is this marked as positive?\n",
    "comment_dict['fq3a3nl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens = []\n",
    "\n",
    "submission = reddit.submission(id=final_id)\n",
    "for top_level_comment in submission.comments:\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    comment = top_level_comment.body\n",
    "    comment = comment.replace('\\n', ' ')\n",
    "    try:\n",
    "        tagged = nltk.pos_tag(comment.split(' '))\n",
    "        tagged_tokens.append(tagged)\n",
    "    except IndexError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = 0\n",
    "adjectives = 0\n",
    "\n",
    "for pair in tagged_tokens:\n",
    "    for tag in pair:\n",
    "        if tag[1] == 'JJ':\n",
    "            adjectives += 1\n",
    "        elif tag[1] == 'NN':\n",
    "            nouns += 1\n",
    "        elif tag[1] == 'NNS':\n",
    "            nouns += 1\n",
    "    \n",
    "\n",
    "print('There are', nouns, 'nouns in this text.')\n",
    "print('There are', adjectives, 'adjectives in this text.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for y in tagged_tokens for x in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_tokens = []\n",
    "\n",
    "submission = reddit.submission(id=final_id)\n",
    "for top_level_comment in submission.comments:\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    reddit_tokens.append(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens = []\n",
    "\n",
    "for token in reddit_tokens:\n",
    "    try:\n",
    "        tagged = nltk.pos_tag(token.split(' '))\n",
    "        tagged_tokens.append(tagged)\n",
    "    except IndexError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toast = sid.polarity_scores(bread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bread = reddit_tokens[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheese = ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.score_valence(cheese, bread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "for comment in reddit_tokens:\n",
    "    print(comment)\n",
    "    ss = sid.polarity_scores(comment)\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}%, '.format(k, round(ss[k]*100, 2)), end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagged_comments = nltk.pos_tag(comment.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [item.replace('.','') for item in wordlist]\n",
    "#     wordlist = [item.replace(':','') for item in wordlist]\n",
    "#     wordlist = [item.replace('-','') for item in wordlist]\n",
    "#     wordlist = [item.replace('?','') for item in wordlist]\n",
    "#     wordlist = [item.replace('!','') for item in wordlist]\n",
    "#     wordlist = [item.replace('*','') for item in wordlist]\n",
    "#     wordlist = [item.replace(',','') for item in wordlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(comment.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens = []\n",
    "\n",
    "submission = reddit.submission(id=final_id)\n",
    "for top_level_comment in submission.comments:\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    comment = top_level_comment.body\n",
    "    comment.replace('\\n', ' ')\n",
    "    try:\n",
    "        tagged = nltk.pos_tag(comment.split(' '))\n",
    "        tagged_tokens.append(tagged)\n",
    "    except IndexError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(submission.comments.body.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = []\n",
    "submission.comments.replace_more(limit=0)\n",
    "for top_level_comment in submission.comments:\n",
    "    wordlist = top_level_comment.body.split()\n",
    "    wordlist = [item.replace('.','') for item in wordlist]\n",
    "    wordlist = [item.replace(':','') for item in wordlist]\n",
    "    wordlist = [item.replace('-','') for item in wordlist]\n",
    "    wordlist = [item.replace('?','') for item in wordlist]\n",
    "    wordlist = [item.replace('!','') for item in wordlist]\n",
    "    wordlist = [item.replace('*','') for item in wordlist]\n",
    "    wordlist = [item.replace(',','') for item in wordlist]\n",
    "    wordlist = [item.replace('(','').replace(')','') for item in wordlist]\n",
    "    allwords.extend(wordlist)\n",
    "    \n",
    "print(allwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = \"allys'on\" \"aren't\"\n",
    "item.find(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting = 'sup homie {}!'\n",
    "names = ['bob', 'joe', 'ally']\n",
    "\n",
    "for name in names:\n",
    "    print(greeting.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"n't\":\n",
    "    \"n't\"\n",
    "else:\n",
    "    item.replace(\"''\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item.replace(\"'\", '').replace('\"', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = []\n",
    "submission.comments.replace_more(limit=0)\n",
    "for top_level_comment in submission.comments:\n",
    "    wordlist = top_level_comment.body.split()\n",
    "    wordlist = [item.replace('.','') for item in wordlist]\n",
    "    wordlist = [item.replace(':','') for item in wordlist]\n",
    "    wordlist = [item.replace('-','') for item in wordlist]\n",
    "    wordlist = [item.replace('?','') for item in wordlist]\n",
    "    wordlist = [item.replace('!','') for item in wordlist]\n",
    "    wordlist = [item.replace('*','') for item in wordlist]\n",
    "    wordlist = [item.replace(',','') for item in wordlist]\n",
    "    wordlist = [item.replace('(','').replace(')','') for item in wordlist]\n",
    "    allwords.extend(wordlist)\n",
    "    \n",
    "print(allwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in allwords:\n",
    "    count = frequency.get(word,0)\n",
    "    frequency[word] = count + 1\n",
    "     \n",
    "frequency_list = frequency.keys()\n",
    " \n",
    "for words in frequency_list:\n",
    "    print (words, frequency [words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = {}\n",
    "for w in wordlist:\n",
    "    count= frequency.get(w,0)\n",
    "    frequency[w]= count + 1\n",
    "    wordfreq.append(wordlist.count(w))\n",
    "\n",
    "frequency_list = frequency.keys()\n",
    "\n",
    "for w in frequency_list:\n",
    "    print (w, frequency[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from praw.models import MoreComments\n",
    "for top_level_comment in submission.comments:\n",
    "    if isinstance(top_level_comment, MoreComments):\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=None)\n",
    "comment_queue = submission.comments[:]  # Seed with top-level\n",
    "while comment_queue:\n",
    "    comment = comment_queue.pop(0)\n",
    "    print(comment.body)\n",
    "    comment_queue.extend(comment.replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=None)\n",
    "for comment in submission.comments.list():\n",
    "    print(comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordstring = top_level_comment.body\n",
    "for top_level_comment in submission.comments:\n",
    "    if isinstance(top_level_comment, MoreComments):\n",
    "        continue"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = []\n",
    "for w in wordlist:\n",
    "    wordfreq.append(wordlist.count(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordstring = 'it was the best of times it was the worst of times '\n",
    "wordstring += 'it was the age of wisdom it was the age of foolishness'\n",
    "\n",
    "wordlist = wordstring.split()\n",
    "\n",
    "wordfreq = []\n",
    "for w in wordlist:\n",
    "    wordfreq.append(wordlist.count(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotsofcomments = post.comments.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecomment = lotsofcomments[0]\n",
    "onecomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecomment.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = reddit.submission(id='3g1jfi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for top_level_comment in submission.comments:\n",
    "    print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text = top_level_comment.body\n",
    "total_analysis = total_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral\n",
    "positive \n",
    "negative \n",
    "compound \n",
    "token_dict \n",
    "score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob = TextBlob(\"This excellent practice sentance is the best one to ues due to it's perfectly written syntax and great message.\")\n",
    "# for np in blob.noun_phrases:\n",
    "#     print(np)\n",
    "\n",
    "# for words, tags in blob.tags:\n",
    "#     print(words, tags)\n",
    "    \n",
    "# for ngram in blob.ngrams(3):\n",
    "#     print(ngram)\n",
    "\n",
    "# blob.sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(comment_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_blob_dict = {}\n",
    "\n",
    "for each_token in tokens:\n",
    "    analysis_tok = TextBlob(each_token)\n",
    "    txt_blob_dict.update({each_token: analysis_tok.sentiment})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok in tokens:\n",
    "    tok.replace(\"\\n\",\"\")\n",
    "    print(tok)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_dict['fq67hfw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.polarity_scores(comment_dict['fq67hfw'].capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for words in TextBlob(comment_dict['fq67hfw']).ngrams(5):\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_score = TextBlob(comment_dict['fq67hfw']).sentiment\n",
    "tb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.replace(\"I\\'m\",\"I am\").replace(\"\\n\",\" \").replace(\"i\\'ll\",\"i will\").replace(\"i\\'m\",\"i will\").lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.polarity_scores(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_score = TextBlob(test).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = NaiveBayesClassifier.train\n",
    "\n",
    "daily_discussion = list(subreddit.hot(limit=1))\n",
    "discussion_id = (str(daily_discussion))\n",
    "final_id = discussion_id.replace(\"[Submission(id='\",\"\").replace(\"')]\",\"\")\n",
    "print(final_id)\n",
    "\n",
    "submission = reddit.submission(id=final_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for running textblob.\n",
    "\n",
    "\n",
    "# blob = TextBlob(\"This excellent practice sentance is the best one to ues due to it's perfectly written syntax and great message.\")\n",
    "# for np in blob.noun_phrases:\n",
    "#     print(np)\n",
    "\n",
    "# for words, tags in blob.tags:\n",
    "#     print(words, tags)\n",
    "    \n",
    "# for ngram in blob.ngrams(3):\n",
    "#     print(ngram)\n",
    "\n",
    "# blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ploting with matplotlib\n",
    "\n",
    "# plt.plot(y, x)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
