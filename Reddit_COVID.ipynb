{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import textblob as textblob\n",
    "import praw\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import creds\n",
    "from pprint import pprint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/AllysonEnglish/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/AllysonEnglish/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/AllysonEnglish/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#Importing NLTK library and associated packaged\n",
    "\n",
    "import nltk\n",
    "nltk.__version__\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag_sents\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#Importing textblob to compare sentiment analysis results with those from nltk\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit API wrapper \n",
    "\n",
    "reddit = praw.Reddit(client_id=creds.client_id, \\\n",
    "                     client_secret=creds.client_secret, \\\n",
    "                     user_agent=creds.user_agent, \\\n",
    "                     username=creds.username, \\\n",
    "                     password=creds.password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting daily ID for the Coronavirus daily discussion post\n",
    "\n",
    "subreddit = reddit.subreddit('Coronavirus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ghnx5y\n"
     ]
    }
   ],
   "source": [
    "daily_discussion = str(list(subreddit.hot(limit=1)))\n",
    "discussion_id = (str(daily_discussion))\n",
    "final_id = discussion_id.replace(\"[Submission(id='\",\"\").replace(\"')]\",\"\")\n",
    "print(final_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "discussion_ids = []\n",
    "\n",
    "for submission in subreddit.search('Daily Discussion Post'):\n",
    "    if 'Daily Discussion Post' in submission.title:\n",
    "        discussion_ids.append(submission.id)\n",
    "    \n",
    "print(len(discussion_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for disc in test:\n",
    "    submission1 = reddit.submission(disc)\n",
    "    \n",
    "    for user_comment in submission1.comments:\n",
    "        submission1.comments.replace_more(limit=0)\n",
    "        print(user_comment.body)\n",
    "        \n",
    "        utc = submission1.created_utc\n",
    "        dt_object = datetime.fromtimestamp(utc)\n",
    "        print(dt_object.strftime(\"%B\"), dt_object.strftime(\"%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "gh1uuu  number of comments:   113\n",
      "gftmeo  number of comments:   128\n",
      "gekees  number of comments:   136\n",
      "ggfrvq  number of comments:   113\n",
      "gf6xnu  number of comments:   121\n",
      "gdbzvl  number of comments:   124\n",
      "gdy3p3  number of comments:   135\n",
      "gcqx67  number of comments:   117\n",
      "ga9u7y  number of comments:   149\n",
      "gaw27w  number of comments:   144\n",
      "g8eo6c  number of comments:   173\n",
      "gbih0c  number of comments:   126\n",
      "g912p1  number of comments:   177\n",
      "gc6tdq  number of comments:   117\n",
      "g61hda  number of comments:   179\n",
      "fjla5p  number of comments:   357\n",
      "g9nks9  number of comments:   121\n",
      "g7tugs  number of comments:   168\n",
      "flad2s  number of comments:   335\n",
      "g6n0i9  number of comments:   159\n",
      "g32elm  number of comments:   178\n",
      "fvz4b2  number of comments:   233\n",
      "g3nu29  number of comments:   158\n",
      "g5fli1  number of comments:   189\n",
      "fj1m3e  number of comments:   357\n",
      "ftmp3u  number of comments:   257\n",
      "fmfa26  number of comments:   333\n",
      "g47znm  number of comments:   167\n",
      "g15wnp  number of comments:   194\n",
      "g2fafj  number of comments:   187\n",
      "fsd9v4  number of comments:   255\n",
      "g1sbbp  number of comments:   192\n",
      "fo5geh  number of comments:   270\n",
      "fhzyu5  number of comments:   346\n",
      "g78qo4  number of comments:   152\n",
      "fn0a59  number of comments:   284\n",
      "fpbug6  number of comments:   247\n",
      "fyg6tm  number of comments:   185\n",
      "flvdic  number of comments:   336\n",
      "fhgnn3  number of comments:   388\n",
      "frr6ik  number of comments:   252\n",
      "fk5m07  number of comments:   308\n",
      "foqmvt  number of comments:   245\n",
      "fve2wq  number of comments:   226\n",
      "fiio5o  number of comments:   337\n",
      "fr66o8  number of comments:   258\n",
      "fnkb5o  number of comments:   314\n",
      "fpxagz  number of comments:   266\n",
      "fz5oik  number of comments:   198\n",
      "ft068e  number of comments:   232\n",
      "fuu28n  number of comments:   221\n",
      "g0jcbi  number of comments:   154\n",
      "g4t5ud  number of comments:   162\n",
      "fwkvfa  number of comments:   209\n",
      "fu8saj  number of comments:   224\n",
      "fxssfx  number of comments:   194\n",
      "fx6u28  number of comments:   194\n",
      "fzx3wk  number of comments:   122\n",
      "fqkk56  number of comments:   214\n",
      "fg9kfi  number of comments:   306\n",
      "ghnx5y  number of comments:   73\n",
      "ffqiyn  number of comments:   263\n",
      "fkpu59  number of comments:   266\n",
      "fgxhiy  number of comments:   220\n",
      "fcr8dy  number of comments:   287\n",
      "fcfg0w  number of comments:   236\n",
      "fe9i00  number of comments:   250\n",
      "ff8roc  number of comments:   263\n",
      "fevfr5  number of comments:   229\n",
      "fdr7u4  number of comments:   238\n",
      "fdcznm  number of comments:   256\n",
      "fb8ygf  number of comments:   257\n",
      "fbuxyx  number of comments:   217\n",
      "fa6etv  number of comments:   194\n",
      "faowou  number of comments:   220\n",
      "f9nx69  number of comments:   164\n",
      "f9az8y  number of comments:   145\n",
      "f7xqas  number of comments:   108\n",
      "f8v592  number of comments:   124\n",
      "f706aq  number of comments:   61\n",
      "f8n49s  number of comments:   67\n",
      "f7h02k  number of comments:   61\n",
      "fere6p  number of comments:   15\n",
      "fgsu67  number of comments:   9\n",
      "fgwgiz  number of comments:   4\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "m_date = []\n",
    "d_date = []\n",
    "time_24 = []\n",
    "comment_ids = []\n",
    "neutral = []\n",
    "positive = []\n",
    "negative = []\n",
    "compound = []\n",
    "discussion_id = final_id\n",
    "comment_dict = {}\n",
    "# token_dict = {}\n",
    "\n",
    "print(len(discussion_ids))\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "for disc_id in discussion_ids:\n",
    "    \n",
    "    submission = reddit.submission(disc_id)\n",
    "    \n",
    "    print(disc_id, \" number of comments:  \", len(submission.comments))\n",
    "    for user_comment in submission.comments:\n",
    "        submission.comments.replace_more(limit=0)\n",
    "\n",
    "        #grab all comment ids\n",
    "        comment_ids.append(str(user_comment))\n",
    "\n",
    "        #grab all comments and responses to comments \n",
    "        #comment_dict.update({str(user_comment): user_comment.body})\n",
    "\n",
    "        #submission.comments.replace_more(limit=0)\n",
    "        comment = user_comment.body\n",
    "        comment.replace('\\n', ' ')\n",
    "        comment.replace('I\\'m', 'i am').replace('i\\'m', 'i am').replace('i\\'ll', 'i will').replace('I\\'ll', 'i will')\n",
    "        comment.lower()\n",
    "        tokens.append(comment)\n",
    "\n",
    "        #grab date/ time info for each comment \n",
    "        utc = submission.created_utc\n",
    "        dt_object = datetime.fromtimestamp(utc)\n",
    "        m_date.append(dt_object.strftime(\"%B\"))\n",
    "        d_date.append(dt_object.strftime(\"%d\"))\n",
    "        time_24.append(dt_object.strftime(\"%X\"))    \n",
    "\n",
    "        #performing sentiment analysis\n",
    "        ss = sid.polarity_scores(comment)\n",
    "        token_dict = {\"comment_body\" : comment}\n",
    "        token_dict.update({\"month\":dt_object.strftime(\"%B\")})\n",
    "        token_dict.update({\"day\" : dt_object.strftime(\"%d\")})\n",
    "        token_dict.update(ss)\n",
    "        negative.append(ss.get('neg'))\n",
    "        positive.append(ss.get('neu'))\n",
    "        neutral.append(ss.get('pos'))\n",
    "        compound.append(ss.get('compound'))\n",
    "\n",
    "        comment_dict.update({str(user_comment): token_dict})\n",
    "\n",
    "with open(r\"/Users/AllysonEnglish/Active/Active/full_dataset_thru_may11.json\", \"w\") as outfile: \n",
    "    json.dump(comment_dict, outfile, indent = 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "neutral = []\n",
    "positive = []\n",
    "negative = []\n",
    "compound = []\n",
    "\n",
    "\n",
    "for comment in tokens:\n",
    "    ss = sid.polarity_scores(comment)\n",
    "    token_dict.update({comment: ss})\n",
    "    negative.append(ss.get('neg'))\n",
    "    positive.append(ss.get('neu'))\n",
    "    neutral.append(ss.get('pos'))\n",
    "    compound.append(ss.get('compound'))\n",
    "    \n",
    "#     for k in sorted(ss):\n",
    "#         print('{0}: {1}%, '.format(k, round(ss[k]*100, 2)), end='')\n",
    "#         print()\n",
    "\n",
    "\n",
    "# sent_dict.update({'sentiment': neg_dict, neu_dict, pos_dict})    \n",
    "# token_dict.pop(\"[removed]\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataframe\n",
    "\n",
    "d = {'discussion_id': comment_ids, 'month': m_date, 'date': d_date, 'time': time_24, 'positive': positive, 'neutral': neutral, 'negative': negative, 'compound': compound}\n",
    "df = pd.DataFrame(data=d)\n",
    "df = df[df.positive != 1.0000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['compound'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(comment_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes a json \n",
    "\n",
    "json_object = json.dumps(comment_dict, indent = 4) \n",
    "  \n",
    "with open(r\"/Users/AllysonEnglish/Active/Active/sent_analysis_covid_reddit3.json\", \"w\") as outfile: \n",
    "    outfile.write(json_object) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df = pd.read_csv(r'/Users/AllysonEnglish/Active/Active/sentiment_analysis_draft1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_add = df[~df['discussion_id'].isin(old_df['discussion_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(r'/Users/AllysonEnglish/Active/Active/sentiment_analysis_draft1.csv', index=False)\n",
    "to_add.to_csv(r'/Users/AllysonEnglish/Active/Active/sentiment_analysis_draft1.csv', mode='a', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sid.polarity_scores(comment_dict['fq0o3e1'])\n",
    "sid.polarity_scores(comment_dict['fq3a3nl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['score'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.loc[['fq3a3nl']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'].count\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = []\n",
    "\n",
    "# for val in df['negative']:\n",
    "#     if df['negative'] >= 0.05:\n",
    "#         score.append(\"negative\")\n",
    "#     else:\n",
    "#         score.append(\"review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_count= 0\n",
    "n_count= 0\n",
    "\n",
    "for value in df['score']:\n",
    "    if value == 'positive':\n",
    "        p_count=p_count+1\n",
    "    elif value == 'negative':\n",
    "        n_count=n_count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "\n",
    "for value in df['negative']:\n",
    "    if value > 0.08:\n",
    "        score.append(\"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each_comment in tokens_2:\n",
    "#     print(each_comment)\n",
    "    pol = sid.polarity_scores(each_comment)\n",
    "    comment_polarity.update({each_comment: pol})\n",
    "#     for j in sorted(pol):\n",
    "#         print('{0}: {1}%, '.format(j, round(pol[j]*100, 2)), end='')\n",
    "#         print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Does anyone have numbers on the recovery rate of hospitalized patients not on ventilators? My 67y mom's test came back + almost 2 weeks ago, but she had to be admitted 2 days ago because her O2 saturation was dropping at home and a serial x-ray was worse. She seems mostly stable right now, but only able to reach 97% saturation on 4L O2 while at rest. I am scared as hell.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = sid.polarity_scores(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing['neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_comments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in comment_polarity.keys():\n",
    "    new_comments.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " comment_polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workdesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens = []\n",
    "\n",
    "submission = reddit.submission(id=final_id)\n",
    "for top_level_comment in submission.comments:\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    comment = top_level_comment.body\n",
    "    comment = comment.replace('\\n', ' ')\n",
    "    try:\n",
    "        tagged = nltk.pos_tag(comment.split(' '))\n",
    "        tagged_tokens.append(tagged)\n",
    "    except IndexError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = 0\n",
    "adjectives = 0\n",
    "\n",
    "for pair in tagged_tokens:\n",
    "    for tag in pair:\n",
    "        if tag[1] == 'JJ':\n",
    "            adjectives += 1\n",
    "        elif tag[1] == 'NN':\n",
    "            nouns += 1\n",
    "        elif tag[1] == 'NNS':\n",
    "            nouns += 1\n",
    "    \n",
    "\n",
    "print('There are', nouns, 'nouns in this text.')\n",
    "print('There are', adjectives, 'adjectives in this text.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for y in tagged_tokens for x in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_tokens = []\n",
    "\n",
    "submission = reddit.submission(id=final_id)\n",
    "for top_level_comment in submission.comments:\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    reddit_tokens.append(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens = []\n",
    "\n",
    "for token in reddit_tokens:\n",
    "    try:\n",
    "        tagged = nltk.pos_tag(token.split(' '))\n",
    "        tagged_tokens.append(tagged)\n",
    "    except IndexError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toast = sid.polarity_scores(bread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bread = reddit_tokens[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheese = ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.score_valence(cheese, bread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "for comment in reddit_tokens:\n",
    "    print(comment)\n",
    "    ss = sid.polarity_scores(comment)\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}%, '.format(k, round(ss[k]*100, 2)), end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagged_comments = nltk.pos_tag(comment.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [item.replace('.','') for item in wordlist]\n",
    "#     wordlist = [item.replace(':','') for item in wordlist]\n",
    "#     wordlist = [item.replace('-','') for item in wordlist]\n",
    "#     wordlist = [item.replace('?','') for item in wordlist]\n",
    "#     wordlist = [item.replace('!','') for item in wordlist]\n",
    "#     wordlist = [item.replace('*','') for item in wordlist]\n",
    "#     wordlist = [item.replace(',','') for item in wordlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(comment.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens = []\n",
    "\n",
    "submission = reddit.submission(id=final_id)\n",
    "for top_level_comment in submission.comments:\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    comment = top_level_comment.body\n",
    "    comment.replace('\\n', ' ')\n",
    "    try:\n",
    "        tagged = nltk.pos_tag(comment.split(' '))\n",
    "        tagged_tokens.append(tagged)\n",
    "    except IndexError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(submission.comments.body.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = []\n",
    "submission.comments.replace_more(limit=0)\n",
    "for top_level_comment in submission.comments:\n",
    "    wordlist = top_level_comment.body.split()\n",
    "    wordlist = [item.replace('.','') for item in wordlist]\n",
    "    wordlist = [item.replace(':','') for item in wordlist]\n",
    "    wordlist = [item.replace('-','') for item in wordlist]\n",
    "    wordlist = [item.replace('?','') for item in wordlist]\n",
    "    wordlist = [item.replace('!','') for item in wordlist]\n",
    "    wordlist = [item.replace('*','') for item in wordlist]\n",
    "    wordlist = [item.replace(',','') for item in wordlist]\n",
    "    wordlist = [item.replace('(','').replace(')','') for item in wordlist]\n",
    "    allwords.extend(wordlist)\n",
    "    \n",
    "print(allwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = \"allys'on\" \"aren't\"\n",
    "item.find(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting = 'sup homie {}!'\n",
    "names = ['bob', 'joe', 'ally']\n",
    "\n",
    "for name in names:\n",
    "    print(greeting.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"n't\":\n",
    "    \"n't\"\n",
    "else:\n",
    "    item.replace(\"''\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item.replace(\"'\", '').replace('\"', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = []\n",
    "submission.comments.replace_more(limit=0)\n",
    "for top_level_comment in submission.comments:\n",
    "    wordlist = top_level_comment.body.split()\n",
    "    wordlist = [item.replace('.','') for item in wordlist]\n",
    "    wordlist = [item.replace(':','') for item in wordlist]\n",
    "    wordlist = [item.replace('-','') for item in wordlist]\n",
    "    wordlist = [item.replace('?','') for item in wordlist]\n",
    "    wordlist = [item.replace('!','') for item in wordlist]\n",
    "    wordlist = [item.replace('*','') for item in wordlist]\n",
    "    wordlist = [item.replace(',','') for item in wordlist]\n",
    "    wordlist = [item.replace('(','').replace(')','') for item in wordlist]\n",
    "    allwords.extend(wordlist)\n",
    "    \n",
    "print(allwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in allwords:\n",
    "    count = frequency.get(word,0)\n",
    "    frequency[word] = count + 1\n",
    "     \n",
    "frequency_list = frequency.keys()\n",
    " \n",
    "for words in frequency_list:\n",
    "    print (words, frequency [words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = {}\n",
    "for w in wordlist:\n",
    "    count= frequency.get(w,0)\n",
    "    frequency[w]= count + 1\n",
    "    wordfreq.append(wordlist.count(w))\n",
    "\n",
    "frequency_list = frequency.keys()\n",
    "\n",
    "for w in frequency_list:\n",
    "    print (w, frequency[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from praw.models import MoreComments\n",
    "for top_level_comment in submission.comments:\n",
    "    if isinstance(top_level_comment, MoreComments):\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=None)\n",
    "comment_queue = submission.comments[:]  # Seed with top-level\n",
    "while comment_queue:\n",
    "    comment = comment_queue.pop(0)\n",
    "    print(comment.body)\n",
    "    comment_queue.extend(comment.replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=None)\n",
    "for comment in submission.comments.list():\n",
    "    print(comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordstring = top_level_comment.body\n",
    "for top_level_comment in submission.comments:\n",
    "    if isinstance(top_level_comment, MoreComments):\n",
    "        continue"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = []\n",
    "for w in wordlist:\n",
    "    wordfreq.append(wordlist.count(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordstring = 'it was the best of times it was the worst of times '\n",
    "wordstring += 'it was the age of wisdom it was the age of foolishness'\n",
    "\n",
    "wordlist = wordstring.split()\n",
    "\n",
    "wordfreq = []\n",
    "for w in wordlist:\n",
    "    wordfreq.append(wordlist.count(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotsofcomments = post.comments.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecomment = lotsofcomments[0]\n",
    "onecomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecomment.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = reddit.submission(id='3g1jfi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for top_level_comment in submission.comments:\n",
    "    print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text = top_level_comment.body\n",
    "total_analysis = total_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral\n",
    "positive \n",
    "negative \n",
    "compound \n",
    "token_dict \n",
    "score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob = TextBlob(\"This excellent practice sentance is the best one to ues due to it's perfectly written syntax and great message.\")\n",
    "# for np in blob.noun_phrases:\n",
    "#     print(np)\n",
    "\n",
    "# for words, tags in blob.tags:\n",
    "#     print(words, tags)\n",
    "    \n",
    "# for ngram in blob.ngrams(3):\n",
    "#     print(ngram)\n",
    "\n",
    "# blob.sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(comment_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_blob_dict = {}\n",
    "\n",
    "for each_token in tokens:\n",
    "    analysis_tok = TextBlob(each_token)\n",
    "    txt_blob_dict.update({each_token: analysis_tok.sentiment})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok in tokens:\n",
    "    tok.replace(\"\\n\",\"\")\n",
    "    print(tok)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_dict['fq67hfw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.polarity_scores(comment_dict['fq67hfw'].capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for words in TextBlob(comment_dict['fq67hfw']).ngrams(5):\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_score = TextBlob(comment_dict['fq67hfw']).sentiment\n",
    "tb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.replace(\"I\\'m\",\"I am\").replace(\"\\n\",\" \").replace(\"i\\'ll\",\"i will\").replace(\"i\\'m\",\"i will\").lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.polarity_scores(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_score = TextBlob(test).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = NaiveBayesClassifier.train\n",
    "\n",
    "daily_discussion = list(subreddit.hot(limit=1))\n",
    "discussion_id = (str(daily_discussion))\n",
    "final_id = discussion_id.replace(\"[Submission(id='\",\"\").replace(\"')]\",\"\")\n",
    "print(final_id)\n",
    "\n",
    "submission = reddit.submission(id=final_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('Coronavirus')\n",
    "daily_discussion = list(subreddit.hot(limit=1))\n",
    "final_id = discussion_id.replace(\"[Submission(id='\",\"\").replace(\"')]\",\"\")\n",
    "print(final_id)\n",
    "\n",
    "submission = reddit.submission(id=final_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coms in subreddit.comments(1589069774, 1589156174):\n",
    "    print(coms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"1485796660\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1589156174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for running textblob.\n",
    "\n",
    "\n",
    "# blob = TextBlob(\"This excellent practice sentance is the best one to ues due to it's perfectly written syntax and great message.\")\n",
    "# for np in blob.noun_phrases:\n",
    "#     print(np)\n",
    "\n",
    "# for words, tags in blob.tags:\n",
    "#     print(words, tags)\n",
    "    \n",
    "# for ngram in blob.ngrams(3):\n",
    "#     print(ngram)\n",
    "\n",
    "# blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ploting with matplotlib\n",
    "\n",
    "# plt.plot(y, x)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
