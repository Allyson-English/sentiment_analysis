{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import textblob as textblob\n",
    "import praw\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import creds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading vader_lexicon: <urlopen error [Errno 8]\n",
      "[nltk_data]     nodename nor servname provided, or not known>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 8] nodename nor\n",
      "[nltk_data]     servname provided, or not known>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 8] nodename nor servname provided, or not\n",
      "[nltk_data]     known>\n"
     ]
    }
   ],
   "source": [
    "#Importing NLTK library and associated packaged\n",
    "\n",
    "import nltk\n",
    "nltk.__version__\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag_sents\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#Importing textblob to compare sentiment analysis results with those from nltk\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit API wrapper \n",
    "\n",
    "reddit = praw.Reddit(client_id=creds.client_id, \\\n",
    "                     client_secret=creds.client_secret, \\\n",
    "                     user_agent=creds.user_agent, \\\n",
    "                     username=creds.username, \\\n",
    "                     password=creds.password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting daily ID for the Coronavirus daily discussion post\n",
    "\n",
    "subreddit = reddit.subreddit('Coronavirus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gh1uuu\n"
     ]
    }
   ],
   "source": [
    "daily_discussion = list(subreddit.hot(limit=1))\n",
    "discussion_id = (str(daily_discussion))\n",
    "final_id = discussion_id.replace(\"[Submission(id='\",\"\").replace(\"')]\",\"\")\n",
    "print(final_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "m_date = []\n",
    "d_date = []\n",
    "time_24 = []\n",
    "comment_ids = []\n",
    "discussion_id = final_id\n",
    "comment_dict = {}\n",
    "\n",
    "\n",
    "submission = reddit.submission(id=final_id)\n",
    "    \n",
    "for user_comment in submission.comments:\n",
    "    submission.comments.replace_more(limit=0)\n",
    "\n",
    "    #grab all comment ids\n",
    "    comment_ids.append(str(user_comment))\n",
    "    \n",
    "    #grab all comments and responses to comments \n",
    "#     comment_dict.update({str(user_comment): user_comment.body})\n",
    "\n",
    "    #submission.comments.replace_more(limit=0)\n",
    "    comment = user_comment.body\n",
    "    comment.replace('\\n', ' ')\n",
    "    comment.replace('I\\'m', 'i am').replace('i\\'m', 'i am').replace('i\\'ll', 'i will').replace('I\\'ll', 'i will')\n",
    "    comment.lower()\n",
    "    tokens.append(comment)\n",
    "\n",
    "    #grab date/ time info for each comment \n",
    "    utc = submission.created_utc\n",
    "    dt_object = datetime.fromtimestamp(utc)\n",
    "    m_date.append(dt_object.strftime(\"%B\"))\n",
    "    d_date.append(dt_object.strftime(\"%d\"))\n",
    "    time_24.append(dt_object.strftime(\"%X\"))    \n",
    "    \n",
    "    #performing sentiment analysis\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    ss = sid.polarity_scores(comment)\n",
    "    token_dict = {\"comment_body\" : comment}\n",
    "    token_dict.update(ss)\n",
    "    \n",
    "    comment_dict.update({str(user_comment): token_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "neutral = []\n",
    "positive = []\n",
    "negative = []\n",
    "compound = []\n",
    "token_dict = {}\n",
    "score = []\n",
    "\n",
    "\n",
    "for comment in tokens:\n",
    "    ss = sid.polarity_scores(comment)\n",
    "    token_dict.update({comment: ss})\n",
    "    negative.append(ss.get('neg'))\n",
    "    positive.append(ss.get('neu'))\n",
    "    neutral.append(ss.get('pos'))\n",
    "    compound.append(ss.get('compound'))\n",
    "    \n",
    "#     for k in sorted(ss):\n",
    "#         print('{0}: {1}%, '.format(k, round(ss[k]*100, 2)), end='')\n",
    "#         print()\n",
    "\n",
    "\n",
    "sent_dict.update({'sentiment': neg_dict, neu_dict, pos_dict})    \n",
    "token_dict.pop(\"[removed]\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataframe\n",
    "\n",
    "d = {'discussion_id': comment_ids, 'month': m_date, 'date': d_date, 'time': time_24, 'positive': positive, 'neutral': neutral, 'negative': negative, 'compound': compound}\n",
    "df = pd.DataFrame(data=d)\n",
    "df = df[df.positive != 1.0000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes a json \n",
    "\n",
    "json_object = json.dumps(comment_dict, indent = 4) \n",
    "  \n",
    "with open(r\"/Users/AllysonEnglish/Active/Active/sent_analysis_covid_reddit2.json\", \"w\") as outfile: \n",
    "    outfile.write(json_object) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df = pd.read_csv(r'/Users/AllysonEnglish/Active/Active/sentiment_analysis_draft1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_add = df[~df['discussion_id'].isin(old_df['discussion_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(r'/Users/AllysonEnglish/Active/Active/sentiment_analysis_draft1.csv', index=False)\n",
    "to_add.to_csv(r'/Users/AllysonEnglish/Active/Active/sentiment_analysis_draft1.csv', mode='a', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sid.polarity_scores(comment_dict['fq0o3e1'])\n",
    "sid.polarity_scores(comment_dict['fq3a3nl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['score'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.loc[['fq3a3nl']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'].count\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = []\n",
    "\n",
    "# for val in df['negative']:\n",
    "#     if df['negative'] >= 0.05:\n",
    "#         score.append(\"negative\")\n",
    "#     else:\n",
    "#         score.append(\"review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_count= 0\n",
    "n_count= 0\n",
    "\n",
    "for value in df['score']:\n",
    "    if value == 'positive':\n",
    "        p_count=p_count+1\n",
    "    elif value == 'negative':\n",
    "        n_count=n_count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "\n",
    "for value in df['negative']:\n",
    "    if value > 0.08:\n",
    "        score.append(\"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each_comment in tokens_2:\n",
    "#     print(each_comment)\n",
    "    pol = sid.polarity_scores(each_comment)\n",
    "    comment_polarity.update({each_comment: pol})\n",
    "#     for j in sorted(pol):\n",
    "#         print('{0}: {1}%, '.format(j, round(pol[j]*100, 2)), end='')\n",
    "#         print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Does anyone have numbers on the recovery rate of hospitalized patients not on ventilators? My 67y mom's test came back + almost 2 weeks ago, but she had to be admitted 2 days ago because her O2 saturation was dropping at home and a serial x-ray was worse. She seems mostly stable right now, but only able to reach 97% saturation on 4L O2 while at rest. I am scared as hell.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = sid.polarity_scores(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing['neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_comments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in comment_polarity.keys():\n",
    "    new_comments.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " comment_polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workdesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens = []\n",
    "\n",
    "submission = reddit.submission(id=final_id)\n",
    "for top_level_comment in submission.comments:\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    comment = top_level_comment.body\n",
    "    comment = comment.replace('\\n', ' ')\n",
    "    try:\n",
    "        tagged = nltk.pos_tag(comment.split(' '))\n",
    "        tagged_tokens.append(tagged)\n",
    "    except IndexError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = 0\n",
    "adjectives = 0\n",
    "\n",
    "for pair in tagged_tokens:\n",
    "    for tag in pair:\n",
    "        if tag[1] == 'JJ':\n",
    "            adjectives += 1\n",
    "        elif tag[1] == 'NN':\n",
    "            nouns += 1\n",
    "        elif tag[1] == 'NNS':\n",
    "            nouns += 1\n",
    "    \n",
    "\n",
    "print('There are', nouns, 'nouns in this text.')\n",
    "print('There are', adjectives, 'adjectives in this text.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for y in tagged_tokens for x in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_tokens = []\n",
    "\n",
    "submission = reddit.submission(id=final_id)\n",
    "for top_level_comment in submission.comments:\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    reddit_tokens.append(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens = []\n",
    "\n",
    "for token in reddit_tokens:\n",
    "    try:\n",
    "        tagged = nltk.pos_tag(token.split(' '))\n",
    "        tagged_tokens.append(tagged)\n",
    "    except IndexError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toast = sid.polarity_scores(bread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bread = reddit_tokens[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheese = ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.score_valence(cheese, bread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "for comment in reddit_tokens:\n",
    "    print(comment)\n",
    "    ss = sid.polarity_scores(comment)\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}%, '.format(k, round(ss[k]*100, 2)), end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagged_comments = nltk.pos_tag(comment.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [item.replace('.','') for item in wordlist]\n",
    "#     wordlist = [item.replace(':','') for item in wordlist]\n",
    "#     wordlist = [item.replace('-','') for item in wordlist]\n",
    "#     wordlist = [item.replace('?','') for item in wordlist]\n",
    "#     wordlist = [item.replace('!','') for item in wordlist]\n",
    "#     wordlist = [item.replace('*','') for item in wordlist]\n",
    "#     wordlist = [item.replace(',','') for item in wordlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(comment.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens = []\n",
    "\n",
    "submission = reddit.submission(id=final_id)\n",
    "for top_level_comment in submission.comments:\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    comment = top_level_comment.body\n",
    "    comment.replace('\\n', ' ')\n",
    "    try:\n",
    "        tagged = nltk.pos_tag(comment.split(' '))\n",
    "        tagged_tokens.append(tagged)\n",
    "    except IndexError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(submission.comments.body.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = []\n",
    "submission.comments.replace_more(limit=0)\n",
    "for top_level_comment in submission.comments:\n",
    "    wordlist = top_level_comment.body.split()\n",
    "    wordlist = [item.replace('.','') for item in wordlist]\n",
    "    wordlist = [item.replace(':','') for item in wordlist]\n",
    "    wordlist = [item.replace('-','') for item in wordlist]\n",
    "    wordlist = [item.replace('?','') for item in wordlist]\n",
    "    wordlist = [item.replace('!','') for item in wordlist]\n",
    "    wordlist = [item.replace('*','') for item in wordlist]\n",
    "    wordlist = [item.replace(',','') for item in wordlist]\n",
    "    wordlist = [item.replace('(','').replace(')','') for item in wordlist]\n",
    "    allwords.extend(wordlist)\n",
    "    \n",
    "print(allwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = \"allys'on\" \"aren't\"\n",
    "item.find(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting = 'sup homie {}!'\n",
    "names = ['bob', 'joe', 'ally']\n",
    "\n",
    "for name in names:\n",
    "    print(greeting.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"n't\":\n",
    "    \"n't\"\n",
    "else:\n",
    "    item.replace(\"''\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item.replace(\"'\", '').replace('\"', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = []\n",
    "submission.comments.replace_more(limit=0)\n",
    "for top_level_comment in submission.comments:\n",
    "    wordlist = top_level_comment.body.split()\n",
    "    wordlist = [item.replace('.','') for item in wordlist]\n",
    "    wordlist = [item.replace(':','') for item in wordlist]\n",
    "    wordlist = [item.replace('-','') for item in wordlist]\n",
    "    wordlist = [item.replace('?','') for item in wordlist]\n",
    "    wordlist = [item.replace('!','') for item in wordlist]\n",
    "    wordlist = [item.replace('*','') for item in wordlist]\n",
    "    wordlist = [item.replace(',','') for item in wordlist]\n",
    "    wordlist = [item.replace('(','').replace(')','') for item in wordlist]\n",
    "    allwords.extend(wordlist)\n",
    "    \n",
    "print(allwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in allwords:\n",
    "    count = frequency.get(word,0)\n",
    "    frequency[word] = count + 1\n",
    "     \n",
    "frequency_list = frequency.keys()\n",
    " \n",
    "for words in frequency_list:\n",
    "    print (words, frequency [words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = {}\n",
    "for w in wordlist:\n",
    "    count= frequency.get(w,0)\n",
    "    frequency[w]= count + 1\n",
    "    wordfreq.append(wordlist.count(w))\n",
    "\n",
    "frequency_list = frequency.keys()\n",
    "\n",
    "for w in frequency_list:\n",
    "    print (w, frequency[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from praw.models import MoreComments\n",
    "for top_level_comment in submission.comments:\n",
    "    if isinstance(top_level_comment, MoreComments):\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=None)\n",
    "comment_queue = submission.comments[:]  # Seed with top-level\n",
    "while comment_queue:\n",
    "    comment = comment_queue.pop(0)\n",
    "    print(comment.body)\n",
    "    comment_queue.extend(comment.replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=None)\n",
    "for comment in submission.comments.list():\n",
    "    print(comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordstring = top_level_comment.body\n",
    "for top_level_comment in submission.comments:\n",
    "    if isinstance(top_level_comment, MoreComments):\n",
    "        continue"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = []\n",
    "for w in wordlist:\n",
    "    wordfreq.append(wordlist.count(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordstring = 'it was the best of times it was the worst of times '\n",
    "wordstring += 'it was the age of wisdom it was the age of foolishness'\n",
    "\n",
    "wordlist = wordstring.split()\n",
    "\n",
    "wordfreq = []\n",
    "for w in wordlist:\n",
    "    wordfreq.append(wordlist.count(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotsofcomments = post.comments.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecomment = lotsofcomments[0]\n",
    "onecomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecomment.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = reddit.submission(id='3g1jfi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for top_level_comment in submission.comments:\n",
    "    print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text = top_level_comment.body\n",
    "total_analysis = total_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral\n",
    "positive \n",
    "negative \n",
    "compound \n",
    "token_dict \n",
    "score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob = TextBlob(\"This excellent practice sentance is the best one to ues due to it's perfectly written syntax and great message.\")\n",
    "# for np in blob.noun_phrases:\n",
    "#     print(np)\n",
    "\n",
    "# for words, tags in blob.tags:\n",
    "#     print(words, tags)\n",
    "    \n",
    "# for ngram in blob.ngrams(3):\n",
    "#     print(ngram)\n",
    "\n",
    "# blob.sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(comment_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_blob_dict = {}\n",
    "\n",
    "for each_token in tokens:\n",
    "    analysis_tok = TextBlob(each_token)\n",
    "    txt_blob_dict.update({each_token: analysis_tok.sentiment})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok in tokens:\n",
    "    tok.replace(\"\\n\",\"\")\n",
    "    print(tok)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_dict['fq67hfw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.polarity_scores(comment_dict['fq67hfw'].capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for words in TextBlob(comment_dict['fq67hfw']).ngrams(5):\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_score = TextBlob(comment_dict['fq67hfw']).sentiment\n",
    "tb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.replace(\"I\\'m\",\"I am\").replace(\"\\n\",\" \").replace(\"i\\'ll\",\"i will\").replace(\"i\\'m\",\"i will\").lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.polarity_scores(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_score = TextBlob(test).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = NaiveBayesClassifier.train\n",
    "\n",
    "daily_discussion = list(subreddit.hot(limit=1))\n",
    "discussion_id = (str(daily_discussion))\n",
    "final_id = discussion_id.replace(\"[Submission(id='\",\"\").replace(\"')]\",\"\")\n",
    "print(final_id)\n",
    "\n",
    "submission = reddit.submission(id=final_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('Coronavirus')\n",
    "daily_discussion = list(subreddit.hot(limit=1))\n",
    "final_id = discussion_id.replace(\"[Submission(id='\",\"\").replace(\"')]\",\"\")\n",
    "print(final_id)\n",
    "\n",
    "submission = reddit.submission(id=final_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coms in subreddit.comments(1589069774, 1589156174):\n",
    "    print(coms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"1485796660\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1589156174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for running textblob.\n",
    "\n",
    "\n",
    "# blob = TextBlob(\"This excellent practice sentance is the best one to ues due to it's perfectly written syntax and great message.\")\n",
    "# for np in blob.noun_phrases:\n",
    "#     print(np)\n",
    "\n",
    "# for words, tags in blob.tags:\n",
    "#     print(words, tags)\n",
    "    \n",
    "# for ngram in blob.ngrams(3):\n",
    "#     print(ngram)\n",
    "\n",
    "# blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ploting with matplotlib\n",
    "\n",
    "# plt.plot(y, x)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
