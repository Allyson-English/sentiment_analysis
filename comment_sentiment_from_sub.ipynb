{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import textblob as textblob\n",
    "import praw\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import creds\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing NLTK library and associated packaged\n",
    "\n",
    "import nltk\n",
    "nltk.__version__\n",
    "# nltk.download('vader_lexicon')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag_sents\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#Importing textblob to compare sentiment analysis results with those from nltk\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulls all discussion ids available and adds\n",
    "\n",
    "def get_disc_ids(subreddit, disc_dict):\n",
    "    \n",
    "    discussion_ids = []\n",
    "    for submission in subreddit.search('Daily Discussion Post'):\n",
    "        if 'Daily Discussion Post' in submission.title:\n",
    "            discussion_ids.append(submission.id)\n",
    "      \n",
    "    for d in discussion_ids:\n",
    "        if d not in disc_dict.keys():\n",
    "            print(\"Added\", d, \"to discussion dictionary.\")\n",
    "            disc_dict[d] = {}\n",
    "        else: \n",
    "            pass\n",
    "    \n",
    "    if len(discussion_ids) != len(disc_dict):\n",
    "        print(\"Alert! Length of discussion_ids does not equal length of dictionary.\")\n",
    "    \n",
    "    return disc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_info(us_com, submission, sid):\n",
    "    comment = us_com.body\n",
    "    comment = comment.replace('\\n', ' ')\n",
    "    comment = comment.replace('I\\'m', 'i am').replace('i\\'m', 'i am').replace('i\\'ll', 'i will').replace('I\\'ll', 'i will')\n",
    "    comment = comment.lower()\n",
    "    \n",
    "    #grab date/ time info for each comment \n",
    "    utc = submission.created_utc\n",
    "    dt_object = datetime.fromtimestamp(utc)  \n",
    "\n",
    "    #performing sentiment analysis\n",
    "    ss = sid.polarity_scores(comment)\n",
    "    \n",
    "    token_dict = {\"comment_body\" : comment}\n",
    "    token_dict.update({\"month\":dt_object.strftime(\"%B\")})\n",
    "    token_dict.update({\"day\" : dt_object.strftime(\"%d\")})\n",
    "    token_dict.update(ss)\n",
    "    \n",
    "    return token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disc_comments(reddit, disc_id):\n",
    "\n",
    "    num = 0\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    st = time()\n",
    "    \n",
    "    for k in disc_id.keys():\n",
    "        num += 1\n",
    "        print(\"Pulling comments for discussion\", num, \"of\", len(disc_id))\n",
    "        submission = reddit.submission(k)\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        \n",
    "        for user_comment in submission.comments:\n",
    "            tok_dict = comment_info(user_comment, submission, sid)\n",
    "            \n",
    "            if str(user_comment) not in disc_id[k].keys():\n",
    "                disc_id[k].setdefault(str(user_comment),tok_dict)\n",
    "        \n",
    "    print(\"\\nProcessing time:\", round((time()-st)/60, 2), \"minutes.\")\n",
    "    return disc_id\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_comments(json_file_path, reddit, subreddit):\n",
    "    \n",
    "    with open(json_file_path, \"r\") as f:\n",
    "        full_comment_dict = json.load(f)\n",
    "    \n",
    "        discussion_ids = get_disc_ids(subreddit, full_comment_dict)\n",
    "        \n",
    "        all_comment_dict = get_disc_comments(reddit, discussion_ids)\n",
    "        \n",
    "        \n",
    "    #update json file with new comments\n",
    "    with open(json_file_path, \"w\") as outfile:\n",
    "        json.dump(all_comment_dict, outfile, indent = 4)\n",
    "        \n",
    "    return all_comment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = r\"/Users/AllysonEnglish/Active/Active/coronavirus_subreddit_full.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling comments for discussion 1 of 86\n",
      "Pulling comments for discussion 2 of 86\n",
      "Pulling comments for discussion 3 of 86\n",
      "Pulling comments for discussion 4 of 86\n",
      "Pulling comments for discussion 5 of 86\n",
      "Pulling comments for discussion 6 of 86\n",
      "Pulling comments for discussion 7 of 86\n",
      "Pulling comments for discussion 8 of 86\n",
      "Pulling comments for discussion 9 of 86\n",
      "Pulling comments for discussion 10 of 86\n",
      "Pulling comments for discussion 11 of 86\n",
      "Pulling comments for discussion 12 of 86\n",
      "Pulling comments for discussion 13 of 86\n",
      "Pulling comments for discussion 14 of 86\n",
      "Pulling comments for discussion 15 of 86\n",
      "Pulling comments for discussion 16 of 86\n",
      "Pulling comments for discussion 17 of 86\n",
      "Pulling comments for discussion 18 of 86\n",
      "Pulling comments for discussion 19 of 86\n",
      "Pulling comments for discussion 20 of 86\n",
      "Pulling comments for discussion 21 of 86\n",
      "Pulling comments for discussion 22 of 86\n",
      "Pulling comments for discussion 23 of 86\n",
      "Pulling comments for discussion 24 of 86\n",
      "Pulling comments for discussion 25 of 86\n",
      "Pulling comments for discussion 26 of 86\n",
      "Pulling comments for discussion 27 of 86\n",
      "Pulling comments for discussion 28 of 86\n",
      "Pulling comments for discussion 29 of 86\n",
      "Pulling comments for discussion 30 of 86\n",
      "Pulling comments for discussion 31 of 86\n",
      "Pulling comments for discussion 32 of 86\n",
      "Pulling comments for discussion 33 of 86\n",
      "Pulling comments for discussion 34 of 86\n",
      "Pulling comments for discussion 35 of 86\n",
      "Pulling comments for discussion 36 of 86\n",
      "Pulling comments for discussion 37 of 86\n",
      "Pulling comments for discussion 38 of 86\n",
      "Pulling comments for discussion 39 of 86\n",
      "Pulling comments for discussion 40 of 86\n",
      "Pulling comments for discussion 41 of 86\n",
      "Pulling comments for discussion 42 of 86\n",
      "Pulling comments for discussion 43 of 86\n",
      "Pulling comments for discussion 44 of 86\n",
      "Pulling comments for discussion 45 of 86\n",
      "Pulling comments for discussion 46 of 86\n",
      "Pulling comments for discussion 47 of 86\n",
      "Pulling comments for discussion 48 of 86\n",
      "Pulling comments for discussion 49 of 86\n",
      "Pulling comments for discussion 50 of 86\n",
      "Pulling comments for discussion 51 of 86\n",
      "Pulling comments for discussion 52 of 86\n",
      "Pulling comments for discussion 53 of 86\n",
      "Pulling comments for discussion 54 of 86\n",
      "Pulling comments for discussion 55 of 86\n",
      "Pulling comments for discussion 56 of 86\n",
      "Pulling comments for discussion 57 of 86\n",
      "Pulling comments for discussion 58 of 86\n",
      "Pulling comments for discussion 59 of 86\n",
      "Pulling comments for discussion 60 of 86\n",
      "Pulling comments for discussion 61 of 86\n",
      "Pulling comments for discussion 62 of 86\n",
      "Pulling comments for discussion 63 of 86\n",
      "Pulling comments for discussion 64 of 86\n",
      "Pulling comments for discussion 65 of 86\n",
      "Pulling comments for discussion 66 of 86\n",
      "Pulling comments for discussion 67 of 86\n",
      "Pulling comments for discussion 68 of 86\n",
      "Pulling comments for discussion 69 of 86\n",
      "Pulling comments for discussion 70 of 86\n",
      "Pulling comments for discussion 71 of 86\n",
      "Pulling comments for discussion 72 of 86\n",
      "Pulling comments for discussion 73 of 86\n",
      "Pulling comments for discussion 74 of 86\n",
      "Pulling comments for discussion 75 of 86\n",
      "Pulling comments for discussion 76 of 86\n",
      "Pulling comments for discussion 77 of 86\n",
      "Pulling comments for discussion 78 of 86\n",
      "Pulling comments for discussion 79 of 86\n",
      "Pulling comments for discussion 80 of 86\n",
      "Pulling comments for discussion 81 of 86\n",
      "Pulling comments for discussion 82 of 86\n",
      "Pulling comments for discussion 83 of 86\n",
      "Pulling comments for discussion 84 of 86\n",
      "Pulling comments for discussion 85 of 86\n",
      "Pulling comments for discussion 86 of 86\n",
      "\n",
      "Processing time: 11.39 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Reddit API wrapper \n",
    "\n",
    "reddit = praw.Reddit(client_id=creds.client_id, \\\n",
    "                     client_secret=creds.client_secret, \\\n",
    "                     user_agent=creds.user_agent, \\\n",
    "                     username=creds.username, \\\n",
    "                     password=creds.password)\n",
    "\n",
    "#Getting daily ID for the Coronavirus daily discussion post\n",
    "\n",
    "subreddit = reddit.subreddit('Coronavirus')\n",
    "\n",
    "updated_dict = add_new_comments(json_file_path, reddit, subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number = 0\n",
    "for k in updated_dict.keys():\n",
    "    for a in updated_dict[k].keys():\n",
    "        number += len(a)\n",
    "number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ghnx5y\n",
      "1 gh1uuu\n",
      "2 gftmeo\n",
      "3 gekees\n",
      "4 ggfrvq\n",
      "5 gf6xnu\n",
      "6 gdbzvl\n",
      "7 gcqx67\n",
      "8 gaw27w\n",
      "9 gdy3p3\n",
      "10 g8eo6c\n",
      "11 gbih0c\n",
      "12 g912p1\n",
      "13 gc6tdq\n",
      "14 ga9u7y\n",
      "15 g61hda\n",
      "16 fjla5p\n",
      "17 flad2s\n",
      "18 g7tugs\n",
      "19 g6n0i9\n",
      "20 g9nks9\n",
      "21 fvz4b2\n",
      "22 g32elm\n",
      "23 g5fli1\n",
      "24 g3nu29\n",
      "25 ftmp3u\n",
      "26 fj1m3e\n",
      "27 fmfa26\n",
      "28 g47znm\n",
      "29 g15wnp\n",
      "30 fsd9v4\n",
      "31 g2fafj\n",
      "32 fn0a59\n",
      "33 g1sbbp\n",
      "34 fo5geh\n",
      "35 fpbug6\n",
      "36 g78qo4\n",
      "37 flvdic\n",
      "38 fyg6tm\n",
      "39 frr6ik\n",
      "40 fk5m07\n",
      "41 foqmvt\n",
      "42 fhgnn3\n",
      "43 fnkb5o\n",
      "44 fr66o8\n",
      "45 fve2wq\n",
      "46 fiio5o\n",
      "47 ft068e\n",
      "48 fz5oik\n",
      "49 fpxagz\n",
      "50 fhzyu5\n",
      "51 fuu28n\n",
      "52 g0jcbi\n",
      "53 g4t5ud\n",
      "54 fwkvfa\n",
      "55 fu8saj\n",
      "56 fxssfx\n",
      "57 fx6u28\n",
      "58 fzx3wk\n",
      "59 fqkk56\n",
      "60 fg9kfi\n",
      "61 ffqiyn\n",
      "62 fkpu59\n",
      "63 fgxhiy\n",
      "64 fcr8dy\n",
      "65 gibmed\n",
      "66 ff8roc\n",
      "67 fe9i00\n",
      "68 fcfg0w\n",
      "69 fevfr5\n",
      "70 fdr7u4\n",
      "71 fdcznm\n",
      "72 fbuxyx\n",
      "73 fb8ygf\n",
      "74 fa6etv\n",
      "75 faowou\n",
      "76 f9nx69\n",
      "77 f9az8y\n",
      "78 f7xqas\n",
      "79 f8v592\n",
      "80 f706aq\n",
      "81 f8n49s\n",
      "82 f7h02k\n",
      "83 fere6p\n",
      "84 fgsu67\n",
      "85 fgwgiz\n"
     ]
    }
   ],
   "source": [
    "for i, k in enumerate(updated_dict.keys()):\n",
    "    print(i, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
