{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import textblob as textblob\n",
    "import praw\n",
    "from datetime import datetime\n",
    "import json\n",
    "import creds\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "#Importing NLTK library and associated packaged\n",
    "\n",
    "import nltk\n",
    "nltk.__version__\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag_sents\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "print(py.__version__)\n",
    "import pandas as pd\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open data file, read in new comments from subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit API wrapper \n",
    "\n",
    "reddit = praw.Reddit(client_id=creds.client_id, \\\n",
    "                     client_secret=creds.client_secret, \\\n",
    "                     user_agent=creds.user_agent, \\\n",
    "                     username=creds.username, \\\n",
    "                     password=creds.password)\n",
    "\n",
    "subreddit = reddit.subreddit('Coronavirus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/allysonenglish/Desktop/workbench/coronavirus_sentanalysis/coronavirus_subcomments.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls all discussion IDS from specified subreddit\n",
    "# Note that default parameters are for all Daily Discussion Posts \n",
    "# To specify a date range include 'Month Date'\n",
    "# To specif a new search term include 'New search criteria' (must come after date)\n",
    "\n",
    "\n",
    "def get_comments(sub, existing_dict, date='', search_term='Daily Discussion Post'):\n",
    "    \n",
    "    full_search = search_term + \" - \" + date\n",
    "    count = 0\n",
    "    \n",
    "    for submission in subreddit.search(full_search):\n",
    "        if search_term in submission.title:\n",
    "            count+=1\n",
    "            if submission.id in existing_dict.keys():\n",
    "                pass\n",
    "            else:\n",
    "                existing_dict.update({submission.id:{}})\n",
    "    print(count, \"discussion ids added to dictionary.\")\n",
    "                \n",
    "    return existing_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_comments(subreddit, data, 'May')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_info(us_com, submission, sid):\n",
    "    \n",
    "    token_dict = {}\n",
    "    \n",
    "    comment = us_com.body\n",
    "    comment = comment.replace('\\n', ' ')\n",
    "    comment = comment.replace('I\\'m', 'i am').replace('i\\'m', 'i am').replace('i\\'ll', 'i will').replace('I\\'ll', 'i will')\n",
    "    comment = comment.lower()\n",
    "    \n",
    "    #grab date/ time info for each comment \n",
    "    utc = submission.created_utc\n",
    "    dt_object = datetime.fromtimestamp(utc)  \n",
    "\n",
    "    #performing sentiment analysis\n",
    "    ss = sid.polarity_scores(comment)\n",
    "    \n",
    "    if comment != '[removed]':\n",
    "        token_dict.update({\"comment_body\" : comment})\n",
    "        token_dict.update({\"month\":dt_object.strftime(\"%B\")})\n",
    "        token_dict.update({\"day\" : dt_object.strftime(\"%d\")})\n",
    "        token_dict.update(ss)\n",
    "    \n",
    "    return token_dict\n",
    "\n",
    "\n",
    "#pulls everything together and returns complete dataset in the form of a dictionary\n",
    "\n",
    "def nltk_sentiment(existing_dict, reddit = reddit):\n",
    "\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    st = time()\n",
    "    tok_dict = {}\n",
    "    count = 0\n",
    "    \n",
    "    for i in existing_dict.keys():\n",
    "        count+= 1\n",
    "        print(\"Fetching comments for Daily Discussion: \", i, \" \", count, \"/\", len(existing_dict.keys()))\n",
    "        existing_dict[i] = {}\n",
    "        submission = reddit.submission(i)\n",
    "        submission.comments.replace_more(limit=0)\n",
    "\n",
    "        for user_comment in submission.comments:\n",
    "            tok_dict = comment_info(user_comment, submission, sid)\n",
    "\n",
    "            if str(user_comment) not in existing_dict[i].keys():\n",
    "                existing_dict[i].setdefault(str(user_comment),tok_dict)\n",
    "        \n",
    "    print(\"\\nProcessing time:\", round((time()-st)/60, 2), \"minutes.\")\n",
    "    \n",
    "    return existing_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes ~12 minutes to run\n",
    "# there doesn't seem to be a more efficient way to ensure all comments from all threads are in dictionary \n",
    "\n",
    "updated_dict = nltk_sentiment(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if there are missing values for keys in dictionary and removes them\n",
    "\n",
    "def remove_missing_values(dictionary):\n",
    "    \"\"\"Comments that have been removed by moderators return empty values in previous function. \n",
    "    This function removes missing values.\"\"\"\n",
    "    \n",
    "    missing = []\n",
    "    d = dictionary.copy()\n",
    "\n",
    "    for k in dictionary.keys():\n",
    "        for a in dictionary[k].keys():\n",
    "            if len(dictionary[k][a]) == 0:\n",
    "                missing.append((k, a))\n",
    "    \n",
    "    for di, ci in missing:\n",
    "        d[di].pop(ci)\n",
    "    \n",
    "    print(len(missing), \"missing values have been removed from this dictionary.\")\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dict = remove_missing_values(updated_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/allysonenglish/Desktop/workbench/coronavirus_sentanalysis/coronavirus_subcomments.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, 'w') as write_file:\n",
    "    json.dump(updated_dict, write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data file if not already active in cell \n",
    "\n",
    "with open(path, \"r\") as read_file:\n",
    "    data = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discussion_id = data.keys()\n",
    "\n",
    "m_date = []\n",
    "d_date = []\n",
    "negative = []\n",
    "neutral = []\n",
    "positive = []\n",
    "compound = []\n",
    "comment_id = []\n",
    "discussion = []\n",
    "\n",
    "for k in data.keys():\n",
    "    \n",
    "    for y in data[k].keys():\n",
    "        discussion.append(k)\n",
    "        comment_id.append(y)\n",
    "    \n",
    "    for x in data[k]:\n",
    "        m_date.append(data[k][x].get('month'))\n",
    "        d_date.append(data[k][x].get('day'))\n",
    "        negative.append(data[k][x].get('neg'))\n",
    "        neutral.append(data[k][x].get('neu'))\n",
    "        positive.append(data[k][x].get('pos'))\n",
    "        compound.append(data[k][x].get('compound'))\n",
    "        \n",
    "d = {'discussion_id' : discussion,'comment_id' : comment_id, 'month': m_date, 'date': d_date, 'positive': positive, 'neutral': neutral, 'negative': negative, 'compound': compound}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_days(x):\n",
    "    if x.startswith('F'):\n",
    "        return 2\n",
    "    elif x.startswith('Mar'):\n",
    "        return 3\n",
    "    elif x.startswith('April'):\n",
    "        return 4\n",
    "    elif x.startswith('May'):\n",
    "        return 5\n",
    "    \n",
    "def sent_classification(x):\n",
    "    if x <= -0.5:\n",
    "        return \"Strong Negative\"\n",
    "    if -0.5 < x <=0:\n",
    "        return \"Negative\"\n",
    "    if 0 < x < 0.5:\n",
    "        return \"Neutral\"\n",
    "    if x >=0.5:\n",
    "        return \"Positive\"\n",
    "    \n",
    "df['DOM'] =df['month'] + \" \" + df['date']\n",
    "\n",
    "df2 = df.set_index([\"DOM\", \"comment_id\"]).count(level=\"DOM\").copy()\n",
    "df2 = df2.reset_index()\n",
    "\n",
    "df2['M'] = df2['DOM'].apply(sort_days)\n",
    "df2['Sent_Class'] = df2['compound'].apply(sent_classification)\n",
    "\n",
    "df2.sort_values(['M', 'DOM'], inplace = True)\n",
    "\n",
    "df2 = df2.reset_index()\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_avrg = df.groupby('DOM').mean().reset_index()\n",
    "\n",
    "daily_avrg['M'] = daily_avrg['DOM'].apply(sort_days)\n",
    "daily_avrg['Sent_Class'] = daily_avrg['compound'].apply(sent_classification)\n",
    "\n",
    "daily_avrg.sort_values(['M', 'DOM'], inplace = True)\n",
    "\n",
    "daily_avrg = daily_avrg.reset_index()\n",
    "\n",
    "daily_avrg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = daily_avrg.merge(df2, left_on='DOM', right_on='DOM')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Scatter(x=daily_avrg['DOM'],\n",
    "                  y=round(daily_avrg['compound'], 4),\n",
    "                   y0='Negative',\n",
    "                  mode='markers',\n",
    "                   name= \"\",\n",
    "                  marker = dict(size=df3['discussion_id']/5, color=daily_avrg['compound'],\n",
    "                                colorscale = 'Portland_r', showscale = True)\n",
    "                  )\n",
    "\n",
    "z = np.polyfit(daily_avrg.index, daily_avrg['compound'], 1)\n",
    "p = np.poly1d(z)\n",
    "\n",
    "trace1 = go.Scatter(x=daily_avrg['DOM'], y=p(daily_avrg.index),\n",
    "                    mode='lines',\n",
    "                    name='Trendline',\n",
    "                    line=dict(color='black'))\n",
    "\n",
    "data = [trace, trace1]\n",
    "\n",
    "layout = {\n",
    "    \"title\": \"Online COVID-19 Sentiment: February Through May 2020\",\n",
    "    'title_x': 0.5,\n",
    "    \"xaxis\": {\n",
    "        \"title\": \"Day\",\n",
    "        \"zeroline\": True,\n",
    "#         \"tickangle\": 0,\n",
    "#         'ticktext': [\n",
    "#             'February',\n",
    "#             'March',\n",
    "#             'April',\n",
    "#             'May'\n",
    "#         ],\n",
    "#         'tickvals': [\n",
    "#             7,\n",
    "#             25,\n",
    "#             37,\n",
    "#             42\n",
    "#         ]\n",
    "    },\n",
    "    \"yaxis\": {\n",
    "        \"title\": \"Sentiment Score\",\n",
    "        \"zeroline\": True,\n",
    "        'zerolinecolor': 'grey',\n",
    "        'zerolinewidth': .169,\n",
    "        'tickmode': 'array',\n",
    "        'ticktext': [\n",
    "            'Negative  ',\n",
    "            'Neutral  ',\n",
    "            'Positive  '\n",
    "        ],\n",
    "        'tickvals': [\n",
    "            -.16,\n",
    "            0,\n",
    "            .12\n",
    "        ]\n",
    "    },\n",
    "    \"showlegend\": False,\n",
    "    'autosize': False,\n",
    "    'width': 1300,\n",
    "    'height': 600,\n",
    "    'paper_bgcolor': 'rgb(255, 255, 255)',\n",
    "    'plot_bgcolor': 'rgb(255, 255, 255)',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "plot({\"data\": data, \"layout\": layout}, output_type='file', include_plotlyjs=True, show_link = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = py.io.to_html({\"data\": data, \"layout\": layout}, include_mathjax=False, full_html=False)\n",
    "\n",
    "with open('/Users/allysonenglish/Desktop/test3sent.html', 'w') as out:\n",
    "    out.write(h)\n",
    "    \n",
    "with open('/Users/allysonenglish/Desktop/test4sent.txt', 'w') as out:\n",
    "    out.write(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
